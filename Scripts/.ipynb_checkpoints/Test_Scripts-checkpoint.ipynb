{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd2bd632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\samur\\anaconda3\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\samur\\anaconda3\\lib\\site-packages (from xgboost) (1.22.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\samur\\anaconda3\\lib\\site-packages (from xgboost) (1.7.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas.core.common import random_state\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "%pip install xgboost\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99e9242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script contains:  \n",
    "#   - prepared data loading,\n",
    "#   - defining custom scoring metric,\n",
    "#   - training and tuning of hyper parameters of such models as (see more in the FittingAndEvaluation notebook):\n",
    "#                                           * Linear Regression,\n",
    "#                                           * Ridge Regression,\n",
    "#                                           * Lasso Regression,\n",
    "#                                           * Elastic Net,\n",
    "#                                           * Decision Tree Regressor,\n",
    "#                                           * Random Forest Regressor, \n",
    "#                                           * XGBoost Regressor.\n",
    "#   - saving models trained on data processed in EDA notebook\n",
    "\n",
    "\n",
    "# Load the processed data\n",
    "def load_processed_data(processed_data_path):\n",
    "    data = pd.read_csv(processed_data_path, index_col = 'Id')\n",
    "    X = data.copy()  \n",
    "    y = X['SalePrice']\n",
    "    X = X.drop(['SalePrice'], axis = 1)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Define a custom scoring method. (See FittingAndEvaluation notebook why this scorer was choosen)\n",
    "\n",
    "# My metric for cross validation\n",
    "def rmse_log(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    return np.sqrt(mean_squared_error(np.log1p(y), np.log1p(y_pred)))\n",
    "\n",
    "# My metric for GridSearch\n",
    "def rmse_log_for_gridsearch(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(np.log(y_true), np.log(y_pred)))\n",
    "\n",
    "custom_scorer = make_scorer(rmse_log_for_gridsearch, greater_is_better=False)\n",
    "\n",
    "\n",
    "# Linear Regression\n",
    "def linear_regression_fit(X, y, rmse_log):\n",
    "    \n",
    "    linear_regression = LinearRegression()\n",
    "\n",
    "    # Get cross-validation score\n",
    "    linear_regression_scores = cross_val_score(linear_regression,\n",
    "                            X,\n",
    "                            y,\n",
    "                            cv = 5,\n",
    "                            scoring = rmse_log)\n",
    "\n",
    "    print(linear_regression_scores.mean())\n",
    "    \n",
    "    return linear_regression\n",
    "\n",
    "\n",
    "# Ridge Regression\n",
    "def ridge_regression_tune_fit(X, y, custom_scorer, alpha = [2,641]):\n",
    "    # Create a model sample\n",
    "    ridge_sample = Ridge()\n",
    "\n",
    "    # Set the search area for GridSearch\n",
    "    ridge_hyper_params = {'alpha': alpha, 'random_state': [0]}\n",
    "\n",
    "    # Create a GridSearch sample and fit the model\n",
    "    ridge_regression = GridSearchCV(ridge_sample, ridge_hyper_params, scoring = custom_scorer, cv = 5)\n",
    "    ridge_regression.fit(X, y)\n",
    "\n",
    "    # Print best parameters and best score\n",
    "    print('Best value of Î»: ', ridge_regression.best_params_)\n",
    "    print('Best score: ', ridge_regression.best_score_)\n",
    "\n",
    "    # Give model best parameters\n",
    "    ridge_best_params = ridge_regression.best_params_\n",
    "    ridge_regression = Ridge(**ridge_best_params)\n",
    "\n",
    "    return ridge_regression\n",
    "\n",
    "\n",
    "\n",
    "# Lasso Regression \n",
    "def lasso_regression_tune_fit(X, y, custom_scorer, alpha = [41]):\n",
    "    # I don't want to overload the output of the Lasso regression\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "    # Create a model sample\n",
    "    lasso_sample = Lasso()\n",
    "\n",
    "    # Set the search area for GridSearch\n",
    "    lasso_hyper_params = {'alpha': alpha, 'random_state': [0]}\n",
    "    \n",
    "    # Create a GridSearch sample and fit the model\n",
    "    lasso_regression = GridSearchCV(lasso_sample, lasso_hyper_params, scoring = custom_scorer, cv = 5)\n",
    "    lasso_regression.fit(X, y)\n",
    "\n",
    "    # Print best parameters and best score\n",
    "    print('best alpha: ', lasso_regression.best_params_)\n",
    "    print('score: ', lasso_regression.best_score_)\n",
    "\n",
    "    # Give model best parameters\n",
    "    lasso_best_params = lasso_regression.best_params_\n",
    "    lasso_regression = Lasso(**lasso_best_params)\n",
    "\n",
    "    return lasso_regression\n",
    "\n",
    "\n",
    "# Elastic Net  \n",
    "def elastic_net_tune_fit(X, y, custom_scorer, alpha = [41], l1_ratio = [1]):\n",
    "    # I don't want to overload the output of the Elastic Net\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "    \n",
    "    # Create a model sample\n",
    "    elastic_net_sample = ElasticNet()\n",
    "\n",
    "    # Set the search area for GridSearch\n",
    "    elnet_hyper_params = {'alpha': alpha, 'l1_ratio': l1_ratio, 'random_state': [0]}\n",
    "\n",
    "    # Create a GridSearch sample and fit the model\n",
    "    elastic_net = GridSearchCV(elastic_net_sample, elnet_hyper_params, scoring = custom_scorer, cv = 5)\n",
    "    elastic_net.fit(X, y)\n",
    "\n",
    "    # Print best parameters and best score\n",
    "    print('best alpha and l1_ratio: ', elastic_net.best_params_)\n",
    "    print('score: ', elastic_net.best_score_)\n",
    "\n",
    "    # Give model best parameters\n",
    "    elnet_best_params = elastic_net.best_params_\n",
    "    elastic_net = ElasticNet(**elnet_best_params)\n",
    "\n",
    "    return elastic_net\n",
    "\n",
    "\n",
    "# DecisionTree   \n",
    "def decision_tree_tune_fit(X, y, custom_scorer, max_depth = [6], min_samples_split = [2], min_samples_leaf = [6], max_features = [35], \n",
    "                           min_impurity_decrease = [0], ccp_alpha = [0]):\n",
    "    # Create a model sample\n",
    "    decision_tree_sample = DecisionTreeRegressor()\n",
    "\n",
    "    # Set the search area for GridSearch\n",
    "    decision_tree_hyper_params = {'max_depth': max_depth,\n",
    "                                'min_samples_split': min_samples_split,\n",
    "                                'min_samples_leaf': min_samples_leaf,\n",
    "                                'max_features': max_features,\n",
    "                                'random_state': [0],\n",
    "                                'min_impurity_decrease': min_impurity_decrease,\n",
    "                                'ccp_alpha': ccp_alpha\n",
    "                                }\n",
    "    \n",
    "    # Create a GridSearch sample and fit the model\n",
    "    decision_tree_regressor = GridSearchCV(decision_tree_sample, decision_tree_hyper_params, \n",
    "                                        scoring = custom_scorer, cv = 5)\n",
    "    decision_tree_regressor.fit(X, y)\n",
    "\n",
    "    # Print best parameters and best score\n",
    "    print('Best DT params: ', decision_tree_regressor.best_params_)\n",
    "    print('Best score: ', decision_tree_regressor.best_score_)\n",
    "\n",
    "    # Give model best parameters\n",
    "    decision_tree_best_params = decision_tree_regressor.best_params_\n",
    "    decision_tree_regressor = DecisionTreeRegressor(**decision_tree_best_params)\n",
    "\n",
    "    return decision_tree_regressor\n",
    "\n",
    "\n",
    "# Random Forest   \n",
    "def random_forest_tune_fit(X, y, custom_scorer, n_estimators = [1150], max_depth = [27], min_samples_split = [3], \n",
    "                           min_samples_leaf = [1], max_features = [12]):\n",
    "    # Create a model sample\n",
    "    random_forest_sample = RandomForestRegressor()\n",
    "\n",
    "    # Set the search area for GridSearch\n",
    "    random_forest_hyper_params = {'n_estimators': n_estimators,\n",
    "                                'max_depth': max_depth, \n",
    "                                'min_samples_split': min_samples_split,\n",
    "                                'min_samples_leaf': min_samples_leaf,\n",
    "                                'max_features': max_features,\n",
    "                                'random_state': [0],\n",
    "                                'n_jobs': [-1]\n",
    "                                }\n",
    "    \n",
    "    # Create a GridSearch sample and fit the model\n",
    "    random_forest_regressor = GridSearchCV(random_forest_sample, random_forest_hyper_params, \n",
    "                                        scoring = custom_scorer, cv = 5)\n",
    "    random_forest_regressor.fit(X, y)\n",
    "\n",
    "    # Print best parameters and best score\n",
    "    print('Best parameters: ', random_forest_regressor.best_params_)\n",
    "    print('Best score: ', random_forest_regressor.best_score_)\n",
    "\n",
    "    # Give model best parameters\n",
    "    random_forest_best_params = random_forest_regressor.best_params_\n",
    "    random_forest_regressor = RandomForestRegressor(**random_forest_best_params)\n",
    "\n",
    "    return random_forest_regressor\n",
    "\n",
    "\n",
    "# # Extreme Gradient Boosting  \n",
    "def xgboost_tune_fit(X, y, custom_scorer, max_depth = [3], min_child_weight = [7], gamma = [0], subsample = [1], colsample_bytree = [1],\n",
    "                     reg_alpha = [0], reg_lambda = [1]):\n",
    "    # Split the data to use eval_set in selecting the best values for n_estimators and learning_rate\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "    # Find the best values for n_estimators and learning_rate\n",
    "    # The best learning_rate was found through experimentation and multiple code executions. Here you can only see the result\n",
    "    xgb_regressor_presearch = XGBRegressor(n_estimators = 1000, learning_rate = 0.25, random_state = 0) \n",
    "    xgb_regressor_presearch.fit(X_train, y_train,\n",
    "                    early_stopping_rounds = 100,\n",
    "                    eval_set = [(X_valid, y_valid)],\n",
    "                    verbose = False)\n",
    "\n",
    "    print(\"Best value for n_estimators: \", xgb_regressor_presearch.best_iteration)\n",
    "\n",
    "    # Create features for best iteretion and best learning rate\n",
    "    xgb_best_iteration = xgb_regressor_presearch.best_iteration\n",
    "    xgb_best_learning_rate = 0.25\n",
    "\n",
    "    # Find the best values for all other parameters using GridSearchCV\n",
    "    # All the values were found through experimentation and multiple code executions. Here you can only see the result\n",
    "    xgb_regressor_sample = XGBRegressor()\n",
    "    xgb_hyper_params = {'n_estimators': [xgb_best_iteration],\n",
    "                        'learning_rate': [xgb_best_learning_rate],\n",
    "                        'max_depth': max_depth, \n",
    "                        'min_child_weight': min_child_weight,  \n",
    "                        'gamma': gamma,\n",
    "                        'subsample': subsample,\n",
    "                        'colsample_bytree': colsample_bytree,\n",
    "                        'reg_alpha': reg_alpha,\n",
    "                        'reg_lambda': reg_lambda,\n",
    "                        'random_state': [0]\n",
    "                    }\n",
    "\n",
    "    # Create a GridSearch sample and fit the model\n",
    "    xgb_regressor = GridSearchCV(xgb_regressor_sample, xgb_hyper_params, scoring = custom_scorer, cv = 10)\n",
    "    xgb_regressor.fit(X, y)\n",
    "\n",
    "    # Give model best parameters\n",
    "    print('Best parameters: ', xgb_regressor.best_params_)\n",
    "    print('Best score: ', xgb_regressor.best_score_)\n",
    "\n",
    "    # Give model best parameters\n",
    "    xgb_best_params = xgb_regressor.best_params_\n",
    "    xgb_regressor = XGBRegressor(**xgb_best_params)\n",
    "\n",
    "    return xgb_regressor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save models\n",
    "def save_models(X, y, rmse_log, custom_scorer):\n",
    "    # Create a list for storing models\n",
    "    models = []\n",
    "\n",
    "    # Add linear regression\n",
    "    models.append(linear_regression_fit(X, y, rmse_log))\n",
    "\n",
    "    # Add other models. (I exclude elastic net, because in my case it's equal to lasso regression)\n",
    "    for function in (ridge_regression_tune_fit, lasso_regression_tune_fit, decision_tree_tune_fit,\n",
    "                random_forest_tune_fit, xgboost_tune_fit):\n",
    "        new_model = function(X, y, custom_scorer)\n",
    "        models.append(new_model)\n",
    "    \n",
    "    # Check that right hyper parameters were saved\n",
    "    print(models)\n",
    "    \n",
    "    # Save models to a file\n",
    "    with open('models.pkl', 'wb') as f:\n",
    "        pickle.dump(models, f)\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5c376c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = '../Data/train_data_processed.csv'\n",
    "\n",
    "X, y = load_processed_data(data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ab81543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11992314746501571\n",
      "Best value of Î»:  {'alpha': 2, 'random_state': 0}\n",
      "Best score:  -0.11924837322151323\n",
      "best alpha:  {'alpha': 41, 'random_state': 0}\n",
      "score:  -0.11832225699293708\n",
      "Best DT params:  {'ccp_alpha': 0, 'max_depth': 6, 'max_features': 35, 'min_impurity_decrease': 0, 'min_samples_leaf': 6, 'min_samples_split': 2, 'random_state': 0}\n",
      "Best score:  -0.14731339342578062\n",
      "Best parameters:  {'max_depth': 27, 'max_features': 12, 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 1150, 'n_jobs': -1, 'random_state': 0}\n",
      "Best score:  -0.11047975622091558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samur\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best value for n_estimators:  18\n",
      "Best parameters:  {'colsample_bytree': 1, 'gamma': 0, 'learning_rate': 0.25, 'max_depth': 3, 'min_child_weight': 7, 'n_estimators': 18, 'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 1, 'subsample': 1}\n",
      "Best score:  -0.11599717485490679\n",
      "[LinearRegression(), Ridge(alpha=2, random_state=0), Lasso(alpha=41, random_state=0), DecisionTreeRegressor(ccp_alpha=0, max_depth=6, max_features=35,\n",
      "                      min_impurity_decrease=0, min_samples_leaf=6,\n",
      "                      random_state=0), RandomForestRegressor(max_depth=27, max_features=12, min_samples_split=3,\n",
      "                      n_estimators=1150, n_jobs=-1, random_state=0), XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None, colsample_bytree=1,\n",
      "             device=None, early_stopping_rounds=None, enable_categorical=False,\n",
      "             eval_metric=None, feature_types=None, gamma=0, grow_policy=None,\n",
      "             importance_type=None, interaction_constraints=None,\n",
      "             learning_rate=0.25, max_bin=None, max_cat_threshold=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=3,\n",
      "             max_leaves=None, min_child_weight=7, missing=nan,\n",
      "             monotone_constraints=None, multi_strategy=None, n_estimators=18,\n",
      "             n_jobs=None, num_parallel_tree=None, random_state=0, ...)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[LinearRegression(),\n",
       " Ridge(alpha=2, random_state=0),\n",
       " Lasso(alpha=41, random_state=0),\n",
       " DecisionTreeRegressor(ccp_alpha=0, max_depth=6, max_features=35,\n",
       "                       min_impurity_decrease=0, min_samples_leaf=6,\n",
       "                       random_state=0),\n",
       " RandomForestRegressor(max_depth=27, max_features=12, min_samples_split=3,\n",
       "                       n_estimators=1150, n_jobs=-1, random_state=0),\n",
       " XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None, colsample_bytree=1,\n",
       "              device=None, early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, feature_types=None, gamma=0, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.25, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=3,\n",
       "              max_leaves=None, min_child_weight=7, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=18,\n",
       "              n_jobs=None, num_parallel_tree=None, random_state=0, ...)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_models(X, y, rmse_log, custom_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69fba3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11992314746501571\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regression_fit(X, y, rmse_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3defa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('linear_regression.pkl', 'rb') as f:\n",
    "    linear_regression = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31c7abcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../Data/test_data_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62a97442",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This LinearRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8708/357144138.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlinear_regression_prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_regression\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m         \"\"\"\n\u001b[1;32m--> 386\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set_intercept\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_scale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36m_decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_decision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"coo\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1461\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"name\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This LinearRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "linear_regression_prediction = linear_regression.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2764626d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_11160/3049604703.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\samur\\AppData\\Local\\Temp/ipykernel_11160/3049604703.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    pip install -r '../requirements.txt'\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def environment_setting():\n",
    "    pip install -r '../requirements.txt'\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.feature_selection import mutual_info_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7713df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
